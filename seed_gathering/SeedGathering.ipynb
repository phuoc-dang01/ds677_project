{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "805e9d4c-f390-4aba-9ec8-627b9ed85aea",
   "metadata": {},
   "source": [
    "### SEED GATHERING GET CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd3e144-73cb-42fa-9550-075c51686a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter_parser import LANGUAGE, make_parser, node_to_string\n",
    "import datasets\n",
    "import os\n",
    "import signal\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import boto3\n",
    "import smart_open\n",
    "from datasets import load_dataset,Dataset\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "def download_contents(blob_id, src_encoding):\n",
    "    s3_url = f\"s3://softwareheritage/content/{blob_id}\"\n",
    "    with smart_open.open(s3_url, \"rb\", compression=\".gz\", transport_params={\"client\": s3}) as fin:\n",
    "        content = fin.read().decode(src_encoding)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAVA_METHOD_QUERY = LANGUAGE.query(\"\"\"\n",
    "# (\n",
    "#   (method_declaration\n",
    "#     name: (identifier) @method.name\n",
    "#     (modifiers)? @method.modifiers\n",
    "#     (type_identifier)? @method.return_type\n",
    "#     parameters: (formal_parameters) @method.parameters) @method.declaration\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "# JAVA_METHOD_QUERY = LANGUAGE.query(\"\"\"\n",
    "# ((method_declaration\n",
    "#     name: (identifier)\n",
    "#     body: (block)\n",
    "#     preceding_comments: (comment_block) @javadoc\n",
    "#   ) @method\n",
    "#   (#match? @javadoc \"^/\\\\*\\\\*[\\\\s\\\\S]*\\\\*/$\")\n",
    "# )\n",
    "# \"\"\")\n",
    "\n",
    "JAVA_METHOD_QUERY = LANGUAGE.query(\"\"\"\n",
    "(\n",
    "    (method_declaration\n",
    "      name: (identifier)\n",
    "      body: (block .\n",
    "        (block_comment\n",
    "            (comment_start) @javadoc.start\n",
    "            (comment_content)\n",
    "            (comment_end) @javadoc.end))) @method.decl\n",
    "    (#eq? @javadoc.start \"/**\")\n",
    "    (#eq? @javadoc.end \"*/\")\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def get_methods(src, tree):\n",
    "    captures = JAVA_METHOD_QUERY.captures(tree.root_node)\n",
    "    res = []\n",
    "    for capture in captures:\n",
    "        node, ty = capture\n",
    "        if ty != \"method.declaration\":\n",
    "            continue\n",
    "        # Filter for top-level methods (starting column 0)\n",
    "        _, col = node.start_point\n",
    "        if col != 0:\n",
    "            continue\n",
    "        res.append(node_to_string(src, node))\n",
    "    return res\n",
    "\n",
    "def parse_ex_java(parser, ex):\n",
    "    ex = download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "    try:\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_methods(buf, tree)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def process_chunk_java(idx_and_chunk):\n",
    "    assert PARSERS is not None\n",
    "    idx, chunk = idx_and_chunk\n",
    "    parser = PARSERS[idx]\n",
    "    chunk_new_methods = set()\n",
    "    for ex in chunk:\n",
    "        chunk_new_methods.update(parse_ex_java(parser, ex))\n",
    "    return chunk_new_methods\n",
    "\n",
    "def main_java(args):\n",
    "    global PARSERS\n",
    "    ds = datasets.load_dataset(\n",
    "        args.dataset,\n",
    "        data_dir=args.data_dir,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    methods = set()\n",
    "    PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "    total_len = len(ds)\n",
    "    CHUNK_SIZE = 1000 * args.num_workers\n",
    "\n",
    "    print(f\"Total length: {total_len}\")\n",
    "    print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "    chunk = []\n",
    "    p = Pool(args.num_workers)\n",
    "    for i, ex in enumerate(ds):\n",
    "        if i % (total_len // 100) == 0:\n",
    "            print(f\"{i}/{total_len}\")\n",
    "        try:\n",
    "            chunk.append(ex)\n",
    "            if len(chunk) == CHUNK_SIZE or i == total_len - 1:\n",
    "                print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "                subchunk_size = len(chunk) // args.num_workers\n",
    "                subchunks = [chunk[i:i + subchunk_size]\n",
    "                             for i in range(0, len(chunk), subchunk_size)]\n",
    "                new_methods_iter = p.imap(\n",
    "                    process_chunk_java, [(i, subchunk) for i, subchunk in enumerate(subchunks)])\n",
    "                print(\"Getting new methods\")\n",
    "                len_before = len(methods)\n",
    "                while True:\n",
    "                    try:\n",
    "                        def timeout_handler(_, __):\n",
    "                            raise KeyboardInterrupt\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(60)\n",
    "                        methods.update(next(new_methods_iter))\n",
    "                        signal.alarm(0)\n",
    "                    except KeyboardInterrupt:\n",
    "                        signal.alarm(0)\n",
    "                        print(\"Keyboard interrupt. Terminating pool\")\n",
    "                        p.terminate()\n",
    "                        p = Pool(args.num_workers)\n",
    "                        break\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "                signal.alarm(0)\n",
    "\n",
    "                PARSERS = [make_parser() for _ in range(args.num_workers)]\n",
    "\n",
    "                print(\n",
    "                    f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(methods) - len_before} new methods\")\n",
    "\n",
    "                chunk = []\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            chunk = []\n",
    "\n",
    "        if i == total_len - 1:\n",
    "            break\n",
    "\n",
    "    p.close()\n",
    "\n",
    "    new_ds_dict = {\n",
    "        \"content\": list(methods),\n",
    "        \"id\": list(range(len(methods)))\n",
    "    }\n",
    "\n",
    "    new_ds = datasets.Dataset.from_dict(new_ds_dict)\n",
    "    # new_ds.push_to_hub(args.push, private=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c058c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: method.declaration, Code: public void sayHello() {\n",
      "        System.out.println(\"Hello, world!\");\n",
      "    }\n",
      "Type: method.name, Code: sayHello\n",
      "Type: method.parameters, Code: ()\n",
      "Type: method.declaration, Code: public int add(int a, int b) {\n",
      "        return a + b;\n",
      "    }\n",
      "Type: method.name, Code: add\n",
      "Type: method.parameters, Code: (int a, int b)\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "public class Example {\n",
    "    public void sayHello() {\n",
    "        System.out.println(\"Hello, world!\");\n",
    "    }\n",
    "    public int add(int a, int b) {\n",
    "        return a + b;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "parser = make_parser()  # Use your make_parser function\n",
    "tree = parser.parse(bytes(code, \"utf8\"))\n",
    "\n",
    "captures = JAVA_METHOD_QUERY.captures(tree.root_node)\n",
    "for node, ty in captures:\n",
    "    print(f\"Type: {ty}, Code: {node_to_string(bytes(code, 'utf8'), node)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74accea3-de2a-4b38-bbf2-b0c7f2be7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMWORKERS = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "166109dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/selfcodealign/seed_gathering'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8798ed1-24c7-4694-a97a-a381ab122392",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"bigcode/the-stack-v2-dedup\", \"Java\", cache_dir=\"../cache\", streaming=True, split=\"train\")\n",
    "\n",
    "data = []\n",
    "n =  2000\n",
    "for i, sample in enumerate(ds):\n",
    "    data.append(sample)\n",
    "    if i >= n:  # Stop after collecting 2000 samples\n",
    "        break\n",
    "\n",
    "map_style_dataset = Dataset.from_list(data)\n",
    "\n",
    "# Verify the Dataset\n",
    "print(map_style_dataset)\n",
    "\n",
    "map_style_dataset.save_to_disk(f\"sampled_dataset_{n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d601380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = Dataset.load_from_disk(\"sampled_dataset_2000\")\n",
    "ds = loaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "491ce0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Override\n",
      "public boolean empApplyReject(String emp_code) {\n",
      "\tlog.info(\"DaoImpl 입사거절\");\n",
      "\tint n = sqlSession.update(NS+\"empApplyReject\",emp_code);\n",
      "\treturn (n>0)?true:false;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = Dataset.load_from_disk(\"/workspace/selfcodealign/datasets/seed2\")\n",
    "print(loaded_dataset['content'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82541f12-dd45-44f7-ad0f-928224086086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 2001\n",
      "Chunk size: 20000\n"
     ]
    }
   ],
   "source": [
    "funs = set()\n",
    "PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "total_len = len(ds)\n",
    "CHUNK_SIZE = 1000 * NUMWORKERS\n",
    "\n",
    "print(f\"Total length: {total_len}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE}\")\n",
    "\n",
    "chunk = []\n",
    "p = Pool(NUMWORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78f242aa-2972-4037-b5d9-acc1f1cc7308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      "Getting new functions\n",
      "list index out of range\n",
      "Done processing chunk 0. Got 39 new functions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'id'],\n",
       "    num_rows: 39\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, ex in enumerate(iter(ds)):\n",
    "    # if i % (total_len // 100) == 0:\n",
    "    #     print(f\"{i}/{total_len}\")\n",
    "    try:\n",
    "        chunk.append(ex)\n",
    "        if len(chunk) == CHUNK_SIZE or i == total_len - 1:\n",
    "            print(f\"Processing chunk {i // CHUNK_SIZE}\")\n",
    "            # divide the chunk into NUM_WORKERS chunks\n",
    "            subchunk_size = len(chunk) // NUMWORKERS\n",
    "            subchunks = [chunk[i:i + subchunk_size]\n",
    "                         for i in range(0, len(chunk), subchunk_size)]\n",
    "            new_funs_iter = p.imap(\n",
    "                process_chunk_java, [(i, subchunk) for i, subchunk in enumerate(subchunks)])\n",
    "            print(\"Getting new functions\")\n",
    "            len_before = len(funs)\n",
    "            while True:\n",
    "                try:\n",
    "                    def timeout_handler(_, __):\n",
    "                        raise KeyboardInterrupt  # it's fineeeeeee\n",
    "                    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                    signal.alarm(60)\n",
    "                    funs.update(next(new_funs_iter))\n",
    "                    signal.alarm(0)\n",
    "                except KeyboardInterrupt:\n",
    "                    signal.alarm(0)\n",
    "                    print(\"Keyboard interrupt. Terminating pool\")\n",
    "                    p.terminate()\n",
    "                    p = Pool(NUMWORKERS)\n",
    "                    break\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            signal.alarm(0)\n",
    "\n",
    "            PARSERS = [make_parser() for _ in range(NUMWORKERS)]\n",
    "\n",
    "            print(\n",
    "                f\"Done processing chunk {i // CHUNK_SIZE}. Got {len(funs) - len_before} new functions\")\n",
    "\n",
    "            chunk = []\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        chunk = []\n",
    "\n",
    "    if i == total_len - 1:\n",
    "        break\n",
    "\n",
    "\n",
    "p.close()\n",
    "new_ds_dict = {\n",
    "    \"content\": list(funs),\n",
    "    \"id\": list(range(len(funs)))\n",
    "}\n",
    "\n",
    "new_ds = datasets.Dataset.from_dict(new_ds_dict)\n",
    "new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d99d75d7-bac9-4f47-bb2c-a5773f46ea18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void promptlist(ActionMapping map, ActionForm actionForm,\n",
      "                       HttpServletRequest req, HttpServletResponse res) {\n",
      "      Users luser = this.getLoginUser(req);\n",
      "      if (luser != null) {\n",
      "            String eventid = StringUtil.getParamValue(req, \"eventid\",\"\");\n",
      "            if(!eventid.equals(\"\")){\n",
      "                List list = ModulesServiceFactory.getProjectAndReportService()\n",
      "                            .getPromptList(eventid);\n",
      "                outputSimpleJsonData(res, this.generatorJsonData(list));\n",
      "            }\n",
      "       }\n",
      "\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ds = new_ds\n",
    "print(ds['content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa60b0-bf47-4b30-93e2-2ee8a71958bf",
   "metadata": {},
   "source": [
    "### SEED GATHERING HIGH-QUALITY SUBSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d3a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dataset = Dataset.load_from_disk(\"sampled_dataset_2000\")\n",
    "ds = loaded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b10a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension', 'filename'],\n",
       "    num_rows: 2001\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b69941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(ex):\n",
    "    download_contents(ex[\"blob_id\"], ex[\"src_encoding\"])\n",
    "    try:\n",
    "        buf = bytes(ex, \"utf8\")\n",
    "        tree = parser.parse(buf)\n",
    "        return get_methods(buf, tree)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "\n",
    "tds = [get_content(_) for _ in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd3300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7f3c414bfd80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3e84b466948e9a940dff116a308a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(\n",
    "    lambda row:{'content': download_contents(row['blob_id'], row['src_encoding'])},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89063770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import signal\n",
    "import hashlib\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "from tree_sitter_parser import LANGUAGE, global_parser\n",
    "\n",
    "# Query to find return statements in Java code\n",
    "RETURN_QUERY = LANGUAGE.query(\"\"\"\n",
    "(return_statement) @return\n",
    "\"\"\")\n",
    "\n",
    "def does_have_return(src: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given Java code contains a return statement with a value.\n",
    "    \"\"\"\n",
    "    tree = global_parser.parse(bytes(src, \"utf8\"))\n",
    "    root = tree.root_node\n",
    "    captures = RETURN_QUERY.captures(root)\n",
    "    for node, _ in captures:\n",
    "        # If it doesn't have an argument, it's not a return with a value\n",
    "        if len(node.children) <= 1:  # Includes \"return\" itself\n",
    "            continue\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def run_javac(directory: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Runs the `javac` command in the given directory and parses the output to count errors for each file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"javac\", \"*.java\"],\n",
    "            cwd=directory,\n",
    "            capture_output=True,\n",
    "            timeout=120,\n",
    "            text=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error running javac: {e}\")\n",
    "        return {}\n",
    "\n",
    "    file_error_map = {}\n",
    "    error_lines = result.stderr.split(\"\\n\")\n",
    "    for line in error_lines:\n",
    "        if line.strip():\n",
    "            parts = line.split(\":\")\n",
    "            if len(parts) >= 2:\n",
    "                file_name = parts[0].strip()\n",
    "                if file_name not in file_error_map:\n",
    "                    file_error_map[file_name] = 0\n",
    "                if \"error\" in line:\n",
    "                    file_error_map[file_name] += 1\n",
    "\n",
    "    return file_error_map\n",
    "\n",
    "def typecheck_batch(files: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Type-checks a batch of Java files and filters out files with compilation errors.\n",
    "    \"\"\"\n",
    "    filemap: Dict[str, str] = {}\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        for content in files:\n",
    "            # Generate a unique filename using SHA-1 hash\n",
    "            hash_object = hashlib.sha1(bytes(content, \"utf8\"))\n",
    "            hex_dig = hash_object.hexdigest()\n",
    "            filemap[hex_dig] = content\n",
    "            file_path = os.path.join(tempdir, hex_dig + \".java\")\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "        # Run javac in the temporary directory\n",
    "        error_map = run_javac(tempdir)\n",
    "        print(error_map)\n",
    "\n",
    "        if not error_map:\n",
    "            return {}\n",
    "\n",
    "        for file_name, error_count in error_map.items():\n",
    "            no_java = file_name.replace(\".java\", \"\")\n",
    "            if error_count > 0 and no_java in filemap:\n",
    "                del filemap[no_java]\n",
    "\n",
    "        print(f\"Pass rate: {len(filemap)}/{len(files)}\")\n",
    "        return filemap\n",
    "\n",
    "# def infer_imports_java(code: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Dummy placeholder for inferring imports in Java code. Java doesn't have a direct equivalent like Python's autoimport.\n",
    "#     \"\"\"\n",
    "#     # For now, this is a no-op since inferring imports in Java is highly context-dependent.\n",
    "#     # Future implementations could leverage static analysis tools for Java.\n",
    "#     print(\"Import inference for Java is not implemented.\")\n",
    "#     return code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cea8d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 3/3\n",
      "Filtered files: {'6b85adf1357e0c708d9451ca258e2da25c056bfc': '\\n    public class Example {\\n        public int add(int a, int b) {\\n            return a + b;\\n        }\\n    }\\n    ', 'd25989fd3fd95dfeadf253e5588abc22fb94faf4': '\\n    public class Example {\\n        public void noReturn() {\\n            System.out.println(\"No return statement here.\");\\n        }\\n    }\\n    ', 'c06cc074eceddad1ee01eb33249f5902aaf01410': '\\n    public class InvalidCode {\\n        public void invalidMethod() {\\n            System.out.println(\"This will not compile\"\\n        }\\n    }\\n    '}\n"
     ]
    }
   ],
   "source": [
    "java_files = [\n",
    "    \"\"\"\n",
    "    public class Example {\n",
    "        public int add(int a, int b) {\n",
    "            return a + b;\n",
    "        }\n",
    "    }\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    public class Example {\n",
    "        public void noReturn() {\n",
    "            System.out.println(\"No return statement here.\");\n",
    "        }\n",
    "    }\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    public class InvalidCode {\n",
    "        public void invalidMethod() {\n",
    "            System.out.println(\"This will not compile\"\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Run type-checking\n",
    "filtered_files = typecheck_batch(java_files)\n",
    "print(f\"Filtered files: {filtered_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5889ead-f07b-460e-8798-37df9b0c8257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to only functions with return statements\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852abf8584f642cd89909f38a6ca6696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=20):   0%|          | 0/2001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['blob_id', 'directory_id', 'path', 'content_id', 'detected_licenses', 'license_type', 'repo_name', 'snapshot_id', 'revision_id', 'branch_name', 'visit_date', 'revision_date', 'committer_date', 'github_id', 'star_events_count', 'fork_events_count', 'gha_license_id', 'gha_event_created_at', 'gha_created_at', 'gha_language', 'src_encoding', 'language', 'is_vendor', 'is_generated', 'length_bytes', 'extension', 'filename', 'content'],\n",
       "    num_rows: 1147\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Filtering to only functions with return statements\")\n",
    "ds = ds.filter(lambda ex: does_have_return(\n",
    "    ex[\"content\"]), num_proc=os.cpu_count())\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8db3fd22-9743-45e8-b17a-2195a5119c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 250/1147 [00:00<00:00, 1069.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n",
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 750/1147 [00:00<00:00, 1145.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1147/1147 [00:01<00:00, 1088.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n",
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 147/147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "max_i = len(ds) - 1\n",
    "\n",
    "new_ds = {\n",
    "    \"content\": [],\n",
    "    \"sha1\": [],\n",
    "    \"id\": [],\n",
    "}\n",
    "\n",
    "e_id = 0\n",
    "for i, ex in enumerate(tqdm(ds, total=len(ds))):\n",
    "    try:\n",
    "        code = ex[\"content\"]\n",
    "\n",
    "        batch.append(code)\n",
    "\n",
    "        if len(batch) == 250 or i == max_i:\n",
    "            filemap = typecheck_batch(batch)\n",
    "            for sha1, contents in filemap.items():\n",
    "                new_ds[\"content\"].append(contents)\n",
    "                new_ds[\"sha1\"].append(sha1)\n",
    "                new_ds[\"id\"].append(e_id)\n",
    "                e_id += 1\n",
    "            batch = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"There was an error: {e}\")\n",
    "        continue\n",
    "\n",
    "new_ds_hf = datasets.Dataset.from_dict(new_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418df388-25a5-460d-825b-c95ede49b43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fb7f3a805d4a33a49a2201ca39d3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = \"../datasets/seed22\"\n",
    "new_ds_hf.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b10a2057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b07d59b4a4448a8e3e8b4f79a01d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5225926"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ds_hf.to_json(\"../datasets/seed22/sample_from_2000.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c67802-a173-4df2-b661-234745f0bfdf",
   "metadata": {},
   "source": [
    "### SEED GATHERING FILTER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ffba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 250/1147 [00:00<00:00, 1045.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 500/1147 [00:00<00:00, 1099.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 750/1147 [00:00<00:00, 1125.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1139/1147 [00:01<00:00, 1096.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 250/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1147/1147 [00:01<00:00, 941.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 1, 'Usage': 0}\n",
      "Pass rate: 147/147\n"
     ]
    }
   ],
   "source": [
    "# if args.infer_imports:\n",
    "#     print(\"Inferring imports for functions\")\n",
    "#     ds = ds.map(lambda ex: {\"content\": infer_imports(\n",
    "#         ex[\"content\"])}, num_proc=os.cpu_count())\n",
    "\n",
    "batch = []\n",
    "max_i = len(ds) - 1\n",
    "\n",
    "new_ds = {\n",
    "    \"content\": [],\n",
    "    \"sha1\": [],\n",
    "    \"id\": [],\n",
    "}\n",
    "\n",
    "e_id = 0\n",
    "for i, ex in enumerate(tqdm(ds, total=len(ds))):\n",
    "    try:\n",
    "        code = ex[\"content\"]\n",
    "        batch.append(code)\n",
    "        if len(batch) == 250 or i == max_i:\n",
    "            filemap = typecheck_batch(batch)\n",
    "            for sha1, contents in filemap.items():\n",
    "                new_ds[\"content\"].append(contents)\n",
    "                new_ds[\"sha1\"].append(sha1)\n",
    "                new_ds[\"id\"].append(e_id)\n",
    "                e_id += 1\n",
    "            batch = []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"There was an error: {e}\")\n",
    "        continue\n",
    "\n",
    "new_ds_hf = datasets.Dataset.from_dict(new_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01086352-e230-4267-9b7b-feab665f681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "from tree_sitter_parser import global_parser, LANGUAGE, does_have_return, make_parser\n",
    "import benchmark_data\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import argparse\n",
    "from vllm import LLM, SamplingParams\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unindent(s):\n",
    "    \"\"\"\n",
    "    Remove leading indentation from a multi-line string.\n",
    "    \"\"\"\n",
    "    lines = s.splitlines()\n",
    "    non_blank_lines = [line for line in lines if line.strip()]\n",
    "    min_indent = min(len(line) - len(line.lstrip())\n",
    "                     for line in non_blank_lines) if non_blank_lines else 0\n",
    "    unindented_lines = [line[min_indent:] if len(\n",
    "        line) >= min_indent else line for line in lines]\n",
    "    return '\\n'.join(unindented_lines)\n",
    "\n",
    "\n",
    "def java_extract_javadoc(code):\n",
    "    \"\"\"\n",
    "    Extract the first Javadoc-style comment (`/** ... */`) from Java code\n",
    "    and return the comment along with the remaining code.\n",
    "\n",
    "    Args:\n",
    "        code (str): The Java code as a string.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The extracted Javadoc comment (str) and the remaining code (str).\n",
    "    \"\"\"\n",
    "    # Find the opening of the Javadoc comment\n",
    "    first_comment_start = code.find(\"/**\")\n",
    "    if first_comment_start == -1:\n",
    "        raise ValueError(\"No Javadoc comment found in the code.\")\n",
    "\n",
    "    # Find the closing of the Javadoc comment\n",
    "    first_comment_end = code.find(\"*/\", first_comment_start)\n",
    "    if first_comment_end == -1:\n",
    "        raise ValueError(\"Javadoc comment is not properly closed.\")\n",
    "\n",
    "    # Extract the comment\n",
    "    comment = code[first_comment_start + 3:first_comment_end]  # Skip `/**` and include content\n",
    "    comment = unindent(comment).strip()  # Unindent and clean up the comment\n",
    "\n",
    "    # Remove the Javadoc comment from the code\n",
    "    remaining_code = code[:first_comment_start] + code[first_comment_end + 2:]\n",
    "\n",
    "    return comment, remaining_code.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4adfc94c-0964-4eaa-9e9c-7749bb4cf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_BLOCK_QUERY = LANGUAGE.query(\"\"\"\n",
    "(method_declaration\n",
    "    body: (block) @method-body)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def template_few_shot(code, answer, rationale):\n",
    "    doc, code = java_extract_javadoc(code)\n",
    "    assert answer == \"No\" or answer == \"Yes\"\n",
    "    prompt = f\"\"\"<issue_start>username_0: I have a function in Java and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```Java\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is: {answer}\n",
    "\n",
    "{rationale}\n",
    "\n",
    "Upvotes: 200\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "FEW_SHOTS = [\n",
    "    (\n",
    "        \"\"\"\n",
    "        public List<String> simpleScanNetwork() {\n",
    "            /**\n",
    "             * Do a simple network scan, which only works if your network configuration\n",
    "             * is 192.168.1.x\n",
    "             */\n",
    "            String baseIp = \"192.168.1.\";\n",
    "            List<String> addresses = new ArrayList<>();\n",
    "            addresses.add(\"127.0.0.1\");\n",
    "\n",
    "            for (int index = 1; index < 255; index++) {\n",
    "                addresses.add(baseIp + index);\n",
    "            }\n",
    "\n",
    "            return addresses;\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"No\",\n",
    "        \"The simpleScanNetwork method you have provided seems to generate addresses that then would be used for a network scan, but does not actually perform it, unlike the method claims.\"\n",
    "    ),\n",
    "    (\n",
    "        \"\"\"\n",
    "        import java.util.*;\n",
    "        \n",
    "        public class DataFrameUtils {\n",
    "            public static DataFrame coerceInteger(DataFrame df) {\n",
    "                /**\n",
    "                 * Loop through the columns of a DataFrame. If it is numeric,\n",
    "                 * convert it to integer and fill missing values with zeros.\n",
    "                 * This is somewhat heavy-handed in an attempt to force\n",
    "                 * systems to recognize sparse columns as integers.\n",
    "                 */\n",
    "                List<String> except = Arrays.asList(\"latitude\", \"longitude\", \"zipCode\");\n",
    "\n",
    "                df.forEachColumn((name, series) -> {\n",
    "                    if (series.isNumeric() && !except.contains(name)) {\n",
    "                        series.fillNaN(0).toInteger();\n",
    "                    }\n",
    "                });\n",
    "\n",
    "                return df;\n",
    "            }\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"Yes\",\n",
    "        \"The docstring does seem to match the implementation! The method loops through the columns of a DataFrame and coerces them as explained.\"\n",
    "    ),\n",
    "    (\n",
    "        \"\"\"\n",
    "        public class NameTransformer {\n",
    "            /**\n",
    "             * Converts a DataFrame to a dictionary.\n",
    "             *\n",
    "             * @param data The input DataFrame.\n",
    "             * @return A map containing transformed names.\n",
    "             */\n",
    "            public static Map<String, Map<String, String>> transformDataFrameToDict(DataFrame data) {\n",
    "                data.setColumn(\"en_name\", data.getColumn(\"en_name\").toUpperCase());\n",
    "                data.setColumn(\"en_name_f\", data.getColumn(\"en_name\").split(\" \")[0]);\n",
    "                data.setColumn(\"en_name_l\", data.getColumn(\"en_name\").split(\" \")[1]);\n",
    "                data.setColumn(\"jp_name_f\", data.getColumn(\"jp_name\").split(\"・\")[0]);\n",
    "                data.setColumn(\"jp_name_l\", data.getColumn(\"jp_name\").split(\"・\")[1]);\n",
    "\n",
    "                Map<String, String> fullNameMap = data.zipToMap(\"en_name\", \"jp_name\");\n",
    "                Map<String, String> firstNameMap = data.zipToMap(\"en_name_f\", \"jp_name_f\");\n",
    "                Map<String, String> lastNameMap = data.zipToMap(\"en_name_l\", \"jp_name_l\");\n",
    "\n",
    "                return Map.of(\n",
    "                    \"fullNameMap\", fullNameMap,\n",
    "                    \"firstNameMap\", firstNameMap,\n",
    "                    \"lastNameMap\", lastNameMap\n",
    "                );\n",
    "            }\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"No\",\n",
    "        \"The transformDataFrameToDict method does indeed convert a DataFrame into a dictionary, but it transforms various columns that were not described in the docstring. For instance, nowhere in the docstring is it mentioned handling Japanese characters or the column names.\"\n",
    "    ),\n",
    "    (\n",
    "        \"\"\"\n",
    "        public double inchesToMeters(double inches) {\n",
    "            /**\n",
    "             * Convert inches to meters.\n",
    "             */\n",
    "            return inches * 0.0254;\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"Yes\",\n",
    "        \"inchesToMeters is a very simple method. The docstring explains concisely its purpose, which is converting inches to meters.\"\n",
    "    ),\n",
    "    (\n",
    "        \"\"\"\n",
    "        public BufferedImage squareCrop(BufferedImage image, Integer targetSize) {\n",
    "            /**\n",
    "             * Crop the image to `targetSize`. If targetSize is null, the image\n",
    "             * is cropped to the smallest dimension, making it square.\n",
    "             */\n",
    "            int width = image.getWidth();\n",
    "            int height = image.getHeight();\n",
    "\n",
    "            if (targetSize == null) {\n",
    "                targetSize = Math.min(width, height);\n",
    "            }\n",
    "\n",
    "            int dx = (width - targetSize) / 2;\n",
    "            int dy = (height - targetSize) / 2;\n",
    "\n",
    "            return image.getSubimage(dx, dy, targetSize, targetSize);\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"Yes\",\n",
    "        \"Following the standard description for docstrings for methods, the squareCrop method description tells exactly what the method does.\"\n",
    "    ),\n",
    "    (\n",
    "        \"\"\"\n",
    "        public Map<String, String> setupMotifFiles(Args args) {\n",
    "            /**\n",
    "             * Convenience method, ensures the setup is the same across\n",
    "             * multiplicity/orientation/spacing workflows.\n",
    "             */\n",
    "            Map<String, String> motifFiles = new HashMap<>();\n",
    "            motifFiles.put(\"early\", String.format(\"%s/%s/ggr.scanmotifs.h5\",\n",
    "                args.getInput(\"inference\").get(args.getCluster()).get(\"scanmotifs_dir\"),\n",
    "                args.getInput(\"inference\").get(args.getCluster()).get(\"scanmotifs_early_dir\")\n",
    "            ));\n",
    "            motifFiles.put(\"mid\", String.format(\"%s/%s/ggr.scanmotifs.h5\",\n",
    "                args.getInput(\"inference\").get(args.getCluster()).get(\"scanmotifs_dir\"),\n",
    "                args.getInput(\"inference\").get(args.getCluster()).get(\"scanmotifs_mid_dir\")\n",
    "            ));\n",
    "            motifFiles.put(\"late\", String.format(\"%s/%s/ggr.scanmotifs.h5\",\n",
    "                args.getInput(\"inference\").get(args.getCluster()).get(\"scanmotifs_dir\"),\n",
    "                args.getInput(\"inference\").get(args.getCluster()).get(\"scanmotifs_late_dir\")\n",
    "            ));\n",
    "            return motifFiles;\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"No\",\n",
    "        \"The docstring for setupMotifFiles just says this is a convenience method. There is definitely not enough information to re-implement this method from the docstring alone.\"\n",
    "    ),\n",
    "    (\n",
    "        \"\"\"\n",
    "        public double trip(double[] u, double[] v) {\n",
    "            /**\n",
    "             * Returns the scalar triple product of vectors u and v and z-axis.\n",
    "             * The convention is z dot (u cross v). Dotting with the z-axis simplifies\n",
    "             * it to the z component of the u cross v.\n",
    "             *\n",
    "             * The product is:\n",
    "             * - positive if v is to the left of u, that is,\n",
    "             *   the shortest right-hand rotation from u to v is ccw.\n",
    "             * - negative if v is to the right of u, that is,\n",
    "             *   the shortest right-hand rotation from u to v is cw.\n",
    "             * - zero if v is collinear with u.\n",
    "             *\n",
    "             * Essentially, trip is the z component of the cross product of u x v.\n",
    "             */\n",
    "            return (u[0] * v[1] - u[1] * v[0]);\n",
    "        }\n",
    "        \"\"\",\n",
    "        \"Yes\",\n",
    "        \"The docstring for the trip method is very detailed and describes the method's purpose and the mathematical formula used to calculate the scalar triple product.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "def prompt_fmt(code):\n",
    "    doc, code = java_extract_javadoc(code)\n",
    "    random.shuffle(FEW_SHOTS)\n",
    "    buf = \"\"\n",
    "    for few in FEW_SHOTS:\n",
    "        buf += template_few_shot(*few)\n",
    "    buf += f\"\"\"<issue_start>username_0: I have a function in Java and I'd like someone to check my description of this function.\n",
    "I'm doing this so that I can write a good docstring for this function.\n",
    "\n",
    "Here is the code for the function:\n",
    "```java\n",
    "{code}\n",
    "```\n",
    "\n",
    "Here is my description of this program:\n",
    "```\n",
    "{doc}\n",
    "```\n",
    "\n",
    "Do not attempt to execute the function or to judge its correctness.\n",
    "Answer with \"Yes\" or \"No\" depending on if my description has enough information alone to re-implement the function.\n",
    "Also, answer with \"No\" if the description does not match the function.\n",
    "Upvotes: 100<issue_comment>username_1: Sure, no problem. I will be able to help.\n",
    "My answer is:\"\"\"\n",
    "    return buf\n",
    "\n",
    "\n",
    "def auto_dtype():\n",
    "    if torch.cuda.is_bf16_supported():\n",
    "        return \"bfloat16\"\n",
    "    return \"auto\"\n",
    "\n",
    "\n",
    "def chunkify(lst, n):\n",
    "    chunks = []\n",
    "    for i in range(0, len(lst), n):\n",
    "        chunk = []\n",
    "        for j in range(n):\n",
    "            if i + j < len(lst):\n",
    "                chunk.append(lst[i + j])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbcfa4b2-d3bb-4560-9578-8ce15d4d68de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'sha1', 'id'],\n",
       "    num_rows: 1147\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = new_ds_hf\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0992371-9ad5-4ad9-8aaa-d74b25eb4da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1147 examples. Running pre-filtering...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(dataset)} examples. Running pre-filtering...\")\n",
    "\n",
    "BAD_WORDS = [\"todo\", \"fixme\", \"bug\"]\n",
    "BAD_IMPORTS = [\n",
    "    \"java.util.Scanner\", \n",
    "    \"java.lang.Runtime\", \n",
    "    \"java.lang.ProcessBuilder\", \n",
    "    \"javax.swing\", \n",
    "    \"java.awt\"\n",
    "]\n",
    "BAD_IMPORTS = [f\"import {b};\" for b in BAD_IMPORTS]\n",
    "# BAD_SUBSTRINGS = BAD_WORDS + BAD_IMPORTS\n",
    "BAD_SUBSTRINGS = BAD_WORDS\n",
    "\n",
    "# bench_filter = benchmark_data.filter_out()\n",
    "# all_bench = bench_filter[\"human_eval_docstrings\"] + \\\n",
    "#     bench_filter[\"human_eval_solutions\"] + \\\n",
    "#     bench_filter[\"mbpp_docstrings\"] + \\\n",
    "#     bench_filter[\"mbpp_solutions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1af077da-feac-4358-8702-e3c06927f0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89770f0f0ce24bf28949be09faa85290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=19):   0%|          | 0/1147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['content', 'sha1', 'id'],\n",
       "    num_rows: 1147\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_filtering_java(ex):\n",
    "    # \"\"\"\n",
    "    # Pre-filter Java code examples based on specific quality criteria.\n",
    "    # \"\"\"\n",
    "    # code = ex[\"content\"]\n",
    "    # code_bytes = code.encode('utf-8')\n",
    "\n",
    "    # # Filter out bad substrings\n",
    "    lower = code.lower()\n",
    "    for word in BAD_SUBSTRINGS:\n",
    "        if word in lower:\n",
    "            return False\n",
    "\n",
    "    # Too many lines of code -- say 150\n",
    "    lines = code.split(\"\\n\")\n",
    "    if len(lines) > 150:\n",
    "        return False\n",
    "\n",
    "    # # Exclude methods without meaningful parameters\n",
    "    # for line in lines:\n",
    "    #     # Look for method declarations\n",
    "    #     if line.strip().startswith((\"public\", \"private\", \"protected\")) and \"()\" in line:\n",
    "    #         return False\n",
    "\n",
    "    # # Filter out methods with no return statement\n",
    "    # parser = make_parser()\n",
    "    # if not does_have_return(code, parser=parser):\n",
    "    #     return False\n",
    "\n",
    "    # try:\n",
    "    #     # Parse the Java code with Tree-sitter\n",
    "    #     tree = global_parser.parse(code_bytes)\n",
    "    #     block, _ = FN_BLOCK_QUERY.captures(tree.root_node)[0]\n",
    "\n",
    "    #     # Get the Javadoc, filter if not a valid Javadoc\n",
    "    #     preceding_comments = block.prev_sibling\n",
    "    #     if not preceding_comments or preceding_comments.type != \"comment\":\n",
    "    #         return False\n",
    "\n",
    "    #     # Extract and validate the Javadoc content\n",
    "    #     docstring_text = preceding_comments.text.decode('utf-8').strip()\n",
    "    #     if not docstring_text.startswith(\"/**\") or not docstring_text.endswith(\"*/\"):\n",
    "    #         return False\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error in filtering: {e}\")\n",
    "    #     return False\n",
    "\n",
    "    return True  # Passes all checks\n",
    "\n",
    "\n",
    "threads = os.cpu_count() - 1  # type: ignore\n",
    "dataset = dataset.filter(pre_filtering_java, num_proc=threads)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39126a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5f47c3874b4355ac039dc778b1b43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5225926"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_json(\"../datasets/seed22/Java_after_pre_filtering.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7afc500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb36ae712b6c412ea74b9c47d55d636c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "dataset = load_dataset(\"json\", data_files=\"../datasets/seed22/Java_after_pre_filtering.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3b14df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content', 'sha1', 'id'],\n",
       "        num_rows: 1147\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bb5f5-d5d8-468f-923e-8e8c94b99e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(f\"../../../StarCoder\", dtype=auto_dtype(),\n",
    "            gpu_memory_utilization=0.95, tensor_parallel_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ec3274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e8a6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5addbba5764949bfeb6bf58ddf8528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afbc968b1f74eb39bef004ba2eac440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ad89523f274a24808e026e71083083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636d50449e7443f59bac1ef1d4b561ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c12ef2ff6f4f7c9efe82a4ae7921a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d5b33d988d42d38c69978d7580d9d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a59bcfa6014c39a3ebde1cfc12827f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPOINT = \"bigcode/starcoder2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
    "model = AutoModelForCausalLM.from_pretrained(CHECKPOINT).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3188670a-403c-4e47-b230-22a3896fe9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running stage 3 filtering on 1 examples...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now running stage 3 filtering on {len(dataset)} examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87ecf24-2e5a-4201-ba6c-cfcae19a9e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unindent(s):\n",
    "    lines = s.splitlines()\n",
    "    non_blank_lines = [line for line in lines if line.strip()]\n",
    "    min_indent = min(len(line) - len(line.lstrip())\n",
    "                     for line in non_blank_lines) if non_blank_lines else 0\n",
    "    unindented_lines = [line[min_indent:] if len(\n",
    "        line) >= min_indent else line for line in lines]\n",
    "    return '\\n'.join(unindented_lines)\n",
    "\n",
    "\n",
    "def py_extract_docstring(code):\n",
    "    first_doc = code.find('\"\"\"')\n",
    "    assert first_doc != -1\n",
    "    first_doc = first_doc + 3\n",
    "    second_doc = code[first_doc+1:].find('\"\"\"')\n",
    "    assert second_doc != -1\n",
    "    second_doc = second_doc + first_doc + 1\n",
    "    doc = code[first_doc:second_doc]\n",
    "    doc = unindent(doc).strip()\n",
    "    code = code[:first_doc-3] + code[second_doc+3:]\n",
    "    return doc, code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93838c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def dummy(): \\n    \"\"\"\\n    \"\"\"\\n pass'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7c13f00-6712-413d-a039-5964f3e7c45f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No Javadoc comment found in the code.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dummy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdef dummy(): \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m pass\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m dummy_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_fmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m few_shot_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m      4\u001b[0m     dummy_prompt)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(dummy))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFew-shot prompt has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfew_shot_toks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 202\u001b[0m, in \u001b[0;36mprompt_fmt\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_fmt\u001b[39m(code):\n\u001b[0;32m--> 202\u001b[0m     doc, code \u001b[38;5;241m=\u001b[39m \u001b[43mjava_extract_javadoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(FEW_SHOTS)\n\u001b[1;32m    204\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m, in \u001b[0;36mjava_extract_javadoc\u001b[0;34m(code)\u001b[0m\n\u001b[1;32m     26\u001b[0m first_comment_start \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/**\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_comment_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Javadoc comment found in the code.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Find the closing of the Javadoc comment\u001b[39;00m\n\u001b[1;32m     31\u001b[0m first_comment_end \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*/\u001b[39m\u001b[38;5;124m\"\u001b[39m, first_comment_start)\n",
      "\u001b[0;31mValueError\u001b[0m: No Javadoc comment found in the code."
     ]
    }
   ],
   "source": [
    "dummy = 'def dummy(): \\n    \"\"\"\\n    \"\"\"\\n pass'\n",
    "dummy_prompt = prompt_fmt(dummy)\n",
    "few_shot_toks = len(tokenizer.encode(\n",
    "    dummy_prompt)) - len(tokenizer.encode(dummy))\n",
    "print(f\"Few-shot prompt has {few_shot_toks} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a4e6e-e67a-4207-9c74-9e2674946387",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for ex in tqdm(dataset, total=len(dataset), desc=\"Generating prompts\"):\n",
    "    code = ex[\"content\"]\n",
    "    toks = len(tokenizer.encode(code)) + few_shot_toks\n",
    "    if toks > 16380:\n",
    "        print(f\"Skipping example with {toks} tokens\")\n",
    "        # to skip, just add dummy prompt\n",
    "        prompts.append(dummy_prompt)\n",
    "        continue\n",
    "    p = prompt_fmt(code)\n",
    "    prompts.append(p)\n",
    "\n",
    "responses = []\n",
    "for chunk in tqdm(chunkify(prompts, 512), desc=\"Generating responses\"):\n",
    "    outs = model.generate(chunk, SamplingParams(\n",
    "        temperature=0.0, stop=\"\\n\", max_tokens=5))\n",
    "    contents = [o.outputs[0].text for o in outs]\n",
    "    for c in contents:\n",
    "        yes_count = c.lower().count(\"yes\")\n",
    "        no_count = c.lower().count(\"no\")\n",
    "        if yes_count > no_count:\n",
    "            responses.append(True)\n",
    "        elif yes_count < no_count:\n",
    "            responses.append(False)\n",
    "        else:\n",
    "            # default to No\n",
    "            responses.append(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02e6021-71c7-449b-a54a-d6f9a54e4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02eef9-f557-4f36-adf3-0115a5c08041",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dataset.select(range(75000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5579e1-7c33-438e-8e79-d8acab10b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da440e82-7104-4c01-99dc-b7dac8803cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = subset.filter(  # horrible hack!\n",
    "    lambda ex, i: responses[i] and \"def dummy()\" not in ex[\"content\"], with_indices=True)\n",
    "print(f\"Filtered {len(dataset) - len(new_ds)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e497d-4afe-4651-b73c-5950e57b06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds.save_to_disk(\"../datasets/seed3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148b29c-1dc5-47a3-bef7-60dc19d75764",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
